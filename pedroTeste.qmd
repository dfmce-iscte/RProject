---
title: "Project"
format: pdf
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(sf)
```

Extract all necessary documents and create a list with all the datasets

```{r}
X2017_S2_NB_FER <- read_delim("2017-2022/2017_S2_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2018_S1_NB_FER <- read_delim("2017-2022/2018_S1_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2018_S2_NB_Fer <- read_delim("2017-2022/2018_S2_NB_Fer.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2019_S1_NB_FER <- read_delim("2017-2022/2019_S1_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2019_S2_NB_FER <- read_delim("2017-2022/2019_S2_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2020_S1_NB_FER <- read_delim("2017-2022/2020_S1_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2020_S2_NB_FER <- read_delim("2017-2022/2020_S2_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2021_S1_NB_FER <- read_delim("2017-2022/2021_S1_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2021_S2_NB_FER <- read_delim("2017-2022/2021_S2_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2022_S1_NB_FER <- read_delim("2017-2022/2022_S1_NB_FER.txt", 
                              delim = "\t", escape_double = FALSE, 
                              trim_ws = TRUE)

X2022_S2_NB_FER <- read_delim("2017-2022/2022_S2_NB_FER.txt", 
                              delim = ";", escape_double = FALSE, trim_ws = TRUE)

X2023_s1_NB_FER <- read.csv("data/validations-reseau-ferre-nombre-validations-par-jour-1er-semestre.csv", sep = ";", header = TRUE)

X2023_s1_NB_FER$JOUR <- as.Date(X2023_s1_NB_FER$JOUR)
# Format 'JOUR' column to the desired format "%d/%m/%Y"
X2023_s1_NB_FER$JOUR <- format(X2023_s1_NB_FER$JOUR, "%d/%m/%Y")

data_frames_list <- list(
  X2017_S2_NB_FER, X2018_S1_NB_FER, X2018_S2_NB_Fer,
  X2019_S1_NB_FER, X2019_S2_NB_FER, X2020_S1_NB_FER, X2020_S2_NB_FER,
  X2021_S1_NB_FER, X2021_S2_NB_FER, X2022_S1_NB_FER, X2022_S2_NB_FER, X2023_s1_NB_FER)

data_frames_list_names <- list("X2017_S2_NB_FER", "X2018_S1_NB_FER", "X2018_S2_NB_Fer","X2019_S1_NB_FER", "X2019_S2_NB_FER", "X2020_S1_NB_FER", "X2020_S2_NB_FER","X2021_S1_NB_FER","X2021_S2_NB_FER", "X2022_S1_NB_FER", "X2022_S2_NB_FER", "X2023_s1_NB_FER")

```

We tried to join all datasets, however some of the columns had more than one data type, therefore transformed all entries into the same data type. Datasets contained the same column "ID_REFA_LDA" but with different labels, therefore relabeled this column in some of the datasets so it had the same label throughout all datasets. When the values from \$CODE_STIF_RES and \$CODE_STIF_ARRET are equal to "ND" the value from LIBELLE_ARRET is "Inconnu": as neither the departure station nor the arrival station are known, it did not make sense to keep these record entries, therefore eliminated these rows Converted the rest of the entries for columns \$CODE_STIF_RES and \$CODE_STIF_ARRET to numerical values. For column \$CATEGORIE_TITRE, verified there were entries with value "?" and entries with value "NON DEFINI": replaced all values "?" to "NON DEFINI". Then it was possible to join all the datasets since they were represented with the same type.

We noticed an inconsistency in the date format, as for all years previous to 2023 the format was DD/MM/YYYY and for 2023 the format was YYYY/MM/DD. As such, we decided to alter the date format for the 2023 entries to match the rest.

```{r}
# Obtain columns with more than one type
results <- list()
for (df in data_frames_list) {
 for (col in names(df)) { 
   col_class <- sapply(df[[col]], class)[1]
   if (!(col %in% names(results))) {
     results[[col]] <- NULL
   }
   results[[col]] <- append(results[[col]], col_class)
 }
}
results <- lapply(results, unique)
multi_class_columns <- names(results)[sapply(results, length) > 1]
print(multi_class_columns)

#Remove the rows with the value "Inconnu" in the column LIBELLE_ARRET
data_frames_list <- lapply(data_frames_list, function(df) {
  subset(df, LIBELLE_ARRET != "Inconnu")
})

#Covert columns to numeric
data_frames_list <- lapply(data_frames_list, function(df) {
  df$CODE_STIF_RES <- as.numeric(df$CODE_STIF_RES)
  df$CODE_STIF_ARRET <- as.numeric(df$CODE_STIF_ARRET)
  return(df)
})

#Rename the columns called "lda" to "ID_REFA_LDA"
data_frames_list <- lapply(data_frames_list, function(df) {
  if ("lda" %in% names(df)) {
    df <- rename(df, ID_REFA_LDA = lda)
  }
  return(df)
})

#Convert JOUR column from 2023 dataset to day/month/year

#Combine all datasets in one 
combined_df <- bind_rows(data_frames_list)

#Replace the registers with "?" in column "CATEGORIE_TITRE" for "NON DEFINI"
combined_df <- mutate(combined_df, CATEGORIE_TITRE = ifelse(CATEGORIE_TITRE == "?", "NON DEFINI", CATEGORIE_TITRE))

```

Convert all entries of column JOUR to the format day/month/year

In order to get a better understanding of the data we are going to explore the existing features by analyzing their distributions.

Firstly, since each line represents the number of passenger validations per day, per stop, and per ticket on the rail network, we realize that to verify if any outlier exists, it is necessary to analyze all distributions based on the NB_VALD feature. 

We start by verifing what is the number of unique values for each column.

```{r}
print(sapply(combined_df, function(x) n_distinct(x)))

```

It is visible that the features CODE_STIF_TRNS, CODE_STIF_RES and CATEGORIE_TITRE are the ones with a small number of unique values so for that we can create a bar plot in order to analyse them.

CODE_STIF_TRNS plot:

```{r}
combined_df$CODE_STIF_TRNS <- factor(combined_df$CODE_STIF_TRNS)

# Group by 'CODE_STIF_TRNS' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- combined_df %>%
  group_by(CODE_STIF_TRNS) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'CODE_STIF_TRNS'
ggplot(sum_by_TRNS, aes(x = CODE_STIF_TRNS, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by CODE_STIF_TRNS", x = "CODE_STIF_TRNS", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

```

Feature "CODE_STIF_TRNS" takes only 3 possible values and after plotting their distribution according to the number of validations, we can see is an imbalance between them as there are considerably more entries for value "100" than for values "800" and "810". However, we did not consider this difference to be problematic as there is still a great number of validations for the last two values.

CODE_STIF_RES plot :

```{r}
combined_df$CODE_STIF_RES <- factor(combined_df$CODE_STIF_RES)

# Group by 'CODE_STIF_RES' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- combined_df %>%
  group_by(CODE_STIF_RES) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'CODE_STIF_RES'
ggplot(sum_by_TRNS, aes(x = CODE_STIF_RES, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by CODE_STIF_RES", x = "CODE_STIF_RES", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

```

Observing the distribution of values for feature "CODE_STIF_RES", we verified a significant discrepancy between the number of passenger validations for value "110" and the number of passenger validations for all other possible values for "CODE_STIF_RES", among which the lowest numbers of validations are for value "800" and "851".

CATEGORIE_TITRE plot:

```{r}


# Group by 'CATEGORIE_TITRE' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- combined_df %>%
  group_by(CATEGORIE_TITRE) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'CATEGORIE_TITRE'
ggplot(sum_by_TRNS, aes(x = CATEGORIE_TITRE, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by CATEGORIE_TITRE", x = "CATEGORIE_TITRE", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))




```

Regarding the distribution of the number of passenger validations for each of the eight different transportation titles, that is, for each value in column "CATEGORIE_TITRE", we can observe there are substantially more validations for the title "NAVIGO" than for the rest, with title "IMAGINE R" coming in second. On the other hand, title "NAVIGO JOUR" shows the lowest number of validations and as such, we consider it to be an outlier.

For columns CODE_STIF_ARRET, LIBELLE_ARRET and ID_REFA_LDA, as the number of unique values was higher and therefore difficulted the visual analysis of the graphics, we plotted two diferent graphs for the top and bottom 5 values with the highest and lowest numbers of passenger validations, respectively.

CODE_STIF_ARRET

```{r}
combined_df$CODE_STIF_ARRET <- factor(combined_df$CODE_STIF_ARRET)

# Group by 'CODE_STIF_ARRET' and calculate the sum of 'NB_VALD' for each group
sum_by_TRNS <- combined_df %>%
  group_by(CODE_STIF_ARRET) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  top_n(5, Sum_NB_VALD)  # Select the top 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the top 5 'CODE_STIF_ARRET' categories
ggplot(sum_by_TRNS, aes(x = reorder(CODE_STIF_ARRET, -Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Top 5 Sum of NB_VALD by CODE_STIF_ARRET", x = "CODE_STIF_ARRET", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))



# Group by 'CODE_STIF_ARRET' and calculate the sum of 'NB_VALD' for each group
sum_by_ARRET <- combined_df %>%
  group_by(CODE_STIF_ARRET) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  arrange(Sum_NB_VALD) %>%
  slice_head(n = 5)  # Select the bottom 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the bottom 5 'CODE_STIF_ARRET' categories
ggplot(sum_by_ARRET, aes(x = reorder(CODE_STIF_ARRET, Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Bottom 5 Sum of NB_VALD by CODE_STIF_ARRET", x = "CODE_STIF_ARRET", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))
```

For column "CODE_STIF_ARRET", the top 5 values with the highest number of validations are "393", "769", "163", "306" and "822", whereas the bottom 5 are "921", "535", "976", "341" and finally "19", which showed a particularly low number of validations.

LIBELLE_ARRET

```{r}


# Group by 'LIBELLE_ARRET' and calculate the sum of 'NB_VALD' for each group
sum_by_TRNS <- combined_df %>%
  group_by(LIBELLE_ARRET) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  top_n(5, Sum_NB_VALD)  # Select the top 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the top 5 'LIBELLE_ARRET' categories
ggplot(sum_by_TRNS, aes(x = reorder(LIBELLE_ARRET, -Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Top 5 Sum of NB_VALD by LIBELLE_ARRET", x = "LIBELLE_ARRET", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))




# Group by 'LIBELLE_ARRET' and calculate the sum of 'NB_VALD' for each group
sum_by_ARRET <- combined_df %>%
  group_by(LIBELLE_ARRET) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  arrange(Sum_NB_VALD) %>%
  slice_head(n = 5)  # Select the bottom 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the bottom 5 'LIBELLE_ARRET' categories
ggplot(sum_by_ARRET, aes(x = reorder(LIBELLE_ARRET, Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Bottom 5 Sum of NB_VALD by LIBELLE_ARRET", x = "LIBELLE_ARRET", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

```

For column "LIBELLE_ARRET", the top 5 values with the highest numer of validations are "SAINT-LAZARE", "LA DEFENSE-GRANDE ARCHE", "GARE DE LYON", "MONTPARNASSE" and "GARE DU NORD", whereas the bottom 5 are "Allée Royale T13", "PLESSIS CHENET", "THIEUX NANTOUIL", "SANTEUIL LE PER" and "MONTGEROULT". There is no big discrepancy in the number of validations among the top 5 values and among the bottom 5 values, but there is a very significant imbalance between the top and bottom, with numbers ranging from 100M to over 200M validations for the top values, and numbers only reaching up to 12.500 validations in the bottom values.

ID_REFA_LDA

```{r}

# Group by 'ID_REFA_LDA' and calculate the sum of 'NB_VALD' for each group
sum_by_TRNS <- combined_df %>%
  group_by(ID_REFA_LDA) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  top_n(5, Sum_NB_VALD)  # Select the top 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the top 5 'ID_REFA_LDA' categories
ggplot(sum_by_TRNS, aes(x = reorder(ID_REFA_LDA, -Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Top 5 Sum of NB_VALD by ID_REFA_LDA", x = "ID_REFA_LDA", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))




# Group by 'ID_REFA_LDA' and calculate the sum of 'NB_VALD' for each group
sum_by_ARRET <- combined_df %>%
  group_by(ID_REFA_LDA) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE)) %>%
  arrange(Sum_NB_VALD) %>%
  slice_head(n = 5)  # Select the bottom 5 categories

# Create a bar plot showing the sum of 'NB_VALD' for the bottom 5 'ID_REFA_LDA' categories
ggplot(sum_by_ARRET, aes(x = reorder(ID_REFA_LDA, Sum_NB_VALD), y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Bottom 5 Sum of NB_VALD by ID_REFA_LDA", x = "ID_REFA_LDA", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))
```

For column "ID_REFA_LDA", the top 5 values with the highest numer of validations are "71517", "73626", "71370", "73794" and "71410", whereas the bottom 5 are "480950", "480951", "59450", "64049" and "425819". Again, there is no big discrepancy in the number of validations among the top 5 values and among the bottom 5 values, but there is a very significant imbalance between the top and bottom, with numbers ranging from 150M to 250M validations for the top values, and numbers reaching only up to 40.000 validations in the bottom values.

=\> nao apagamos os outliers porque queremos ver inconsistencias e diferenças ao longo do tempo

=\> Uma vez que temos muitos dados e que oos pcs nao tem capacidade para correr tantos dados optamos por obter 100000 dados com a mesma distribuiçao temporal dos dados.

```{r}
# Calculate the proportion of records for each YearMonth
combined_df <- combined_df %>%
  mutate(JOUR = as.Date(JOUR, format = "%d/%m/%Y"), 
         YearMonth = format(JOUR, "%m/%Y"))
combined_df

n <- nrow(combined_df)

# Perform stratified sampling
set.seed(123) # for reproducibility
subset_df <- combined_df %>%
  group_by(YearMonth) %>%
  sample_n(size = (n()/n) * 100000 )


# Now, subset_df is your subset data frame with 100,000 records
subset_df
```

Aggregate the dataset at the "Zone d'arrêt" level defined by the ID_REFA_LDA feature, collect geographical data about their localisiations

```{r}

data <- st_read("data/stations/REF_ZdA/PL_ZDL_R_06_12_2023.shp")
data <- st_transform(data, crs = 4326)
data <- dplyr::rename(data, ID_REFA_LDA = "idrefa_lda")
subset_df$ID_REFA_LDA <- as.numeric(subset_df$ID_REFA_LDA)
final_df <- left_join(subset_df, data, by = "ID_REFA_LDA")
write.table(final_df, file = "final.csv", sep = ";", row.names = FALSE)
```

```{r}
final_df <- data.frame(lapply(final_df, function(x) gsub("\n|\r", " ", x)))

# Now write the dataframe to a file
write.table(final_df, file = "final.csv", sep = ";", row.names = FALSE)
```

Corram so esta

```{r}
final <- read.csv("final.csv", sep = ";", header = TRUE)
final
```

### 2. Exploratory Data Analysis (EDA)

Create a 3 columns: one for month, other for Year and for Season

```{r}
# Extract month, year, and season information
final <- final %>%
  mutate(JOUR = as.Date(JOUR, format = "%Y-%m-%d"), 
         Month = format(JOUR, "%m"),
         Year = format(JOUR, "%Y"),
         Season = case_when(
           between(as.numeric(format(JOUR, "%m")), 3, 5) ~ "Spring",
           between(as.numeric(format(JOUR, "%m")), 6, 8) ~ "Summer",
           between(as.numeric(format(JOUR, "%m")), 9, 11) ~ "Autumn",
           TRUE ~ "Winter"
         ))

```

```{r}
final
```

```{r}


# Group by 'Year' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- final %>%
  group_by(Year) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'Year'
ggplot(sum_by_TRNS, aes(x = Year, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by Year", x = "Year", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))

```

```{r}

# Group by 'Month' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- final%>%
  group_by(Month) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'Month'
ggplot(sum_by_TRNS, aes(x = Month, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by Month", x = "Month", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))
```

```{r}
# Group by 'Season' and calculate the sum of 'NB_VALID' for each group
sum_by_TRNS <- final %>%
  group_by(Season) %>%
  summarise(Sum_NB_VALD = sum(NB_VALD, na.rm = TRUE))

# Create a bar plot showing the sum of 'NB_VALD' for each 'Season'
ggplot(sum_by_TRNS, aes(x = Season, y = Sum_NB_VALD)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Sum of NB_VALD by Season", x = "Season", y = "Sum of NB_VALD") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  scale_y_continuous(labels = function(x) format(x, scientific = FALSE))
```
